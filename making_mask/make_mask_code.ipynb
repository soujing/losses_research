{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 코드는 같은 연구실에서 연구중인 안혜성님과 김승현님의 도움을 받았습니다.\n",
    "* Thanks To Hye-seong An\n",
    "* Thanks To Seung-hyun Kim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8rAU9PR_6LB"
   },
   "source": [
    "# 라이브러리로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch  # os.environ 다음에 import해야 적용돼요\n",
    "\n",
    "# files and system\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output\n",
    "# working with images\n",
    "import cv2\n",
    "import imageio\n",
    "import scipy.ndimage\n",
    "# import skimage.transform\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import notebook\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# losses\n",
    "from metrics_loss_0521 import *\n",
    "\n",
    "import mmcv\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수\n",
    "- 정확도 측정 함수 (iou, dice, precision, recall, f1)\n",
    "- 데이터 로드\n",
    "- 모델 로드\n",
    "- 손실함수 로드\n",
    "- 모델 학습 및 이미지 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_segmentation(outputs: torch.Tensor, labels: torch.Tensor, metric, batch_output=False):\n",
    "    # outputs가 dict이거나 tuple일 경우, tensor를 가져옴\n",
    "    if isinstance(outputs, dict):\n",
    "        outputs = outputs['out']\n",
    "    elif isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "\n",
    "    # sigmoid를 적용하여 예측값을 확률(0~1)로 변환\n",
    "    # 만약, 모델에 sigmoid나 이에 상응하는 활성화 함수가 포함되어 있으면 아래 줄을 주석 처리할 것\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "\n",
    "    # 픽셀 별 예측 값을 0.5를 기준으로 0 또는 1로 thresholding\n",
    "    outputs = outputs > 0.5\n",
    "\n",
    "    # binary class의 경우 출력 channel은 1이므로, (BATCH, 1, H, W)의 형식을 가짐\n",
    "    # 따라서, (BATCH, 1, H, W) -> (BATCH, H, W)로 차원을 줄여줌\n",
    "    outputs = outputs.squeeze(1).byte()  # (BATCH, 1, H, W) -> (BATCH, H, W)\n",
    "    labels = labels.squeeze(1).byte()    # (BATCH, 1, H, W) -> (BATCH, H, W)\n",
    "\n",
    "    # SMOOTH는 나눗셈에서 분모가 0인 것을 방지하기 위해 더해주는 값\n",
    "    SMOOTH = 1e-8\n",
    "\n",
    "    if metric == 'iou':\n",
    "        # IoU : intersection / union\n",
    "        intersection = (outputs & labels).float().sum((1, 2))  # (BATCH, H, W)에서 픽셀 단위로 AND 연산 후 합산\n",
    "        union = (outputs | labels).float().sum((1, 2))         # (BATCH, H, W)에서 픽셀 단위로 OR 연산 후 합산\n",
    "        result = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    elif metric == 'dice':\n",
    "        # Dice Coefficient: 2 * intersection / (output + label)\n",
    "        intersection = (outputs & labels).float().sum((1, 2))  # (BATCH, H, W)에서 픽셀 단위로 AND 연산 후 합산\n",
    "        result = (2 * intersection + SMOOTH) / (outputs.float().sum((1, 2)) + labels.float().sum((1, 2)) + SMOOTH)\n",
    "    elif metric == 'precision':\n",
    "        # Precision: TP / (TP + FP)\n",
    "        true_positive = (outputs & labels).float().sum((1, 2))  # True Positive (TP): 예측과 실제가 모두 1인 경우\n",
    "        predicted_positive = outputs.float().sum((1, 2))        # Predicted Positive: 예측이 1인 경우\n",
    "        result = (true_positive + SMOOTH) / (predicted_positive + SMOOTH)\n",
    "    elif metric == 'recall':\n",
    "        # Recall: TP / (TP + FN)\n",
    "        true_positive = (outputs & labels).float().sum((1, 2))  # True Positive (TP): 예측과 실제가 모두 1인 경우\n",
    "        actual_positive = labels.float().sum((1, 2))            # Actual Positive: 실제가 1인 경우\n",
    "        result = (true_positive + SMOOTH) / (actual_positive + SMOOTH)\n",
    "    elif metric == 'f1':\n",
    "        # F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        true_positive = (outputs & labels).float().sum((1, 2))  # True Positive (TP): 예측과 실제가 모두 1인 경우\n",
    "        predicted_positive = outputs.float().sum((1, 2))        # Predicted Positive: 예측이 1인 경우\n",
    "        actual_positive = labels.float().sum((1, 2))            # Actual Positive: 실제가 1인 경우\n",
    "        # Precision과 Recall 계산\n",
    "        precision = (true_positive + SMOOTH) / (predicted_positive + SMOOTH)\n",
    "        recall = (true_positive + SMOOTH) / (actual_positive + SMOOTH)\n",
    "        # F1 Score 계산\n",
    "        result = (2 * precision * recall) / (precision + recall + SMOOTH)  \n",
    "\n",
    "    if batch_output:\n",
    "        return result  # shape: [BATCH] : 배치 내 각 이미지별 값 (벡터)\n",
    "    else:\n",
    "        return result.mean()  # shape: float (단일 상수 값) : 배치 내 모든 이미지의 평균 (상수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model/FCBformer/pvt_v2.py:331: UserWarning: Overwriting pvt_v2_b0 in registry with FCBformer.pvt_v2.pvt_v2_b0. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model/FCBformer/pvt_v2.py:342: UserWarning: Overwriting pvt_v2_b1 in registry with FCBformer.pvt_v2.pvt_v2_b1. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model/FCBformer/pvt_v2.py:353: UserWarning: Overwriting pvt_v2_b2 in registry with FCBformer.pvt_v2.pvt_v2_b2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model/FCBformer/pvt_v2.py:363: UserWarning: Overwriting pvt_v2_b3 in registry with FCBformer.pvt_v2.pvt_v2_b3. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model/FCBformer/pvt_v2.py:374: UserWarning: Overwriting pvt_v2_b4 in registry with FCBformer.pvt_v2.pvt_v2_b4. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model/FCBformer/pvt_v2.py:385: UserWarning: Overwriting pvt_v2_b5 in registry with FCBformer.pvt_v2.pvt_v2_b5. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model/FCBformer/pvt_v2.py:396: UserWarning: Overwriting pvt_v2_b2_li in registry with FCBformer.pvt_v2.pvt_v2_b2_li. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCBFormer load가능\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def need_load_data(data_name, split_ratio, random_seed):\n",
    "    df = pd.read_csv('/project/ahnailab/jsj0414/지역포함_0113/working_path/ISIC_path_change_ver_3.csv')\n",
    "    image_files = list(df[df['type'] == data_name]['images'])\n",
    "    label_files = list(df[df['type'] == data_name]['labels'])\n",
    "\n",
    "    if len(split_ratio) == 2:  # split_ratio = (train, test)\n",
    "        test_size = split_ratio[1] / sum(split_ratio)\n",
    "        train_images, test_images, train_labels, test_labels = train_test_split(image_files, label_files, test_size=test_size, random_state=random_seed)\n",
    "        return train_images, test_images, train_labels, test_labels\n",
    "        \n",
    "    elif len(split_ratio) == 3:  # split_ratio = (train, validation, test)\n",
    "        trainval2test_size = split_ratio[2] / sum(split_ratio)\n",
    "        trainval_images, test_images, trainval_labels, test_labels = train_test_split(image_files, label_files, test_size=trainval2test_size, random_state=random_seed)\n",
    "        train2val_size = split_ratio[1] / sum(split_ratio[0:2])\n",
    "        train_images, val_images, train_labels, val_labels = train_test_split(trainval_images, trainval_labels, test_size=train2val_size, random_state=random_seed)\n",
    "        return train_images, val_images, test_images, train_labels, val_labels, test_labels\n",
    "    else:\n",
    "        img = df[df['type'] == data_name]['images']\n",
    "        lab = df[df['type'] == data_name]['labels']\n",
    "        return _, _, img, _, _, lab\n",
    "        \n",
    "\n",
    "def init_model(model_name):\n",
    "    model = None  # 기본값을 None으로 설정하여 변수가 초기화되지 않는 상황 방지\n",
    "    \n",
    "    if model_name == 'FCBFormer':\n",
    "        sys.path.append(\"/project/ahnailab/jsj0414/지역포함_0113/working_path/model/\")\n",
    "        from FCBmodels import FCBFormer\n",
    "        model = FCBFormer(size=224)\n",
    "        \n",
    "    # 모델이 None인 경우, 예외 처리\n",
    "    if model is None:\n",
    "        raise ValueError(f\"모델 이름 '{model_name}'이 잘못되었거나 모델을 로드할 수 없습니다.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "models = ['FCBFormer'] # ['ColonFormer','DuckNet','UNet++','Unet','ESFPNet','DeepLab_V3+','FCN','ColonSegNet', 'EMCADNet','FCBFormer','caranet','FAT_Net']  # , 'EMCADNet','FCBFormer','caranet','FAT_Net'\n",
    "for model in models:\n",
    "    try: \n",
    "        init_model(model)\n",
    "        print(f'{model} load가능\\n')\n",
    "    except Exception as e:\n",
    "        print(f'{model} 로드 중 에러발생, 모델 로드 불가: {e}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_loss(loss_name):\n",
    "    dic_name2func={\n",
    "        'IoULoss' : IoULoss,\n",
    "        'DiceLoss' : DiceLoss,\n",
    "        'BCELoss' : BCELoss,\n",
    "        'FocalLoss' : FocalLoss,\n",
    "        'IoUDiceLoss' : IoUDiceLoss,\n",
    "        'IoUBCELoss' : IoUBCELoss,\n",
    "        'IoUFocalLoss' : IoUFocalLoss,\n",
    "        'DiceBCELoss' : DiceBCELoss,\n",
    "        'DiceFocalLoss' : DiceFocalLoss,\n",
    "        'BCEFocalLoss' : BCEFocalLoss,\n",
    "        'IoUWithTVLoss' : IoUWithTVLoss,\n",
    "        \"IoUWithTV_tar_Loss\" : IoUWithTV_tar_Loss,\n",
    "        \"IoUWithSSIMLoss\" : IoUWithSSIMLoss,\n",
    "        \"SpatiallyWeightedLoss\" : SpatiallyWeightedLoss,\n",
    "        \"IoUWithEdgeLoss\" : IoUWithEdgeLoss,\n",
    "        \"IoUWithContextualLoss\" : IoUWithContextualLoss,\n",
    "        \"DiceWithTVLoss_in\" : DiceWithTVLoss_in,\n",
    "        \"DiceWithTVLoss_tar\" : DiceWithTVLoss_tar,\n",
    "        \"MultiScaleDiceLoss\" : MultiScaleDiceLoss,\n",
    "        \"DiceWithContextualLoss\" : DiceWithContextualLoss,\n",
    "        \"DiceWithEdgeLoss\" : DiceWithEdgeLoss,\n",
    "        \"DiceWithSSIMLoss\" : DiceWithSSIMLoss,\n",
    "        \"FocalWithTVLoss_in\" : FocalWithTVLoss_in,\n",
    "        \"FocalWithTVLoss_tar\" : FocalWithTVLoss_tar,\n",
    "        \"FocalWithRegionWeighting\" : FocalWithRegionWeighting,\n",
    "        \"FocalWithGaussianLoss\" : FocalWithGaussianLoss,\n",
    "        \"BCEWithSSIMLoss\" : BCEWithSSIMLoss,\n",
    "        \"BCEWithTVLoss\" : BCEWithTVLoss,\n",
    "        \"BCEWithTVLoss_tar\" : BCEWithTVLoss_tar,\n",
    "        # adding\n",
    "        'FocalWithContextualLoss' : FocalWithContextualLoss, \n",
    "        'BCEWithContextualLoss' : BCEWithContextualLoss, \n",
    "        'FocalWithSSIMLoss': FocalWithSSIMLoss, \n",
    "        'FocalWithEdgeLoss' : FocalWithEdgeLoss, \n",
    "        'BCEWithEdgeLoss' : BCEWithEdgeLoss,\n",
    "        'IoUWithGaussianLoss' : IoUWithGaussianLoss, \n",
    "        'BCEWithGaussianLoss' : BCEWithGaussianLoss, \n",
    "        'DiceWithGaussianLoss' : DiceWithGaussianLoss, \n",
    "        'DiceWeightedLoss' : DiceWeightedLoss, \n",
    "        'BCEWeightedLoss' : BCEWeightedLoss, \n",
    "        'IoUWeightedLoss' : IoUWeightedLoss\n",
    "    }\n",
    "    return dic_name2func[loss_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(data_names, model_name, loss_names, split_ratio=[0.6, 0.2, 0.2], base_random_seed=42, epochs=200, patience=50, BATCH_SIZE=8):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for n_data, data_name in enumerate(data_names):\n",
    "        if base_random_seed != None:\n",
    "            random_seed = base_random_seed\n",
    "            random.seed(random_seed)\n",
    "            np.random.seed(random_seed)\n",
    "            torch.manual_seed(random_seed)\n",
    "            torch.cuda.manual_seed(random_seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        train_images, val_images, test_images, train_labels, val_labels, test_labels = load_data(data_name, split_ratio=split_ratio, random_seed=random_seed)\n",
    "\n",
    "        custom_dataset_train = myDataSet(train_images, train_labels, transforms=test_transforms)\n",
    "        custom_dataset_val = myDataSet(val_images, val_labels, transforms=test_transforms)\n",
    "        custom_dataset_test = myDataSet(test_images, test_labels, transforms=test_transforms)\n",
    "       \n",
    "        dataloader_train = torch.utils.data.DataLoader(custom_dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "        dataloader_val = torch.utils.data.DataLoader(custom_dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        dataloader_test = torch.utils.data.DataLoader(custom_dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        \n",
    "        model = init_model(model_name)\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        for n_loss, loss_name in enumerate(loss_names):\n",
    "            print(f\"Start the task training_model : {n_data+1}'s data is {data_name} and {n_loss+1}'s loss function is {loss_name}\")\n",
    "            criterion = load_loss(loss_name)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 1e-8)                    \n",
    "\n",
    "            state = {'best_val_dice' : 0, 'best_val_loss' : np.inf, 'best_epoch' : 0, 'last_epoch' : -1}    \n",
    "        \n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for imgs, masks in dataloader_train:\n",
    "                    if len(imgs) == 1: continue\n",
    "                        \n",
    "                    imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "                    prediction = model(imgs)\n",
    "\n",
    "                    if isinstance(prediction, dict):\n",
    "                        prediction = torch.Tensor(prediction['out'])\n",
    "                    elif isinstance(prediction, tuple):\n",
    "                        prediction = torch.Tensor(prediction[0])\n",
    "                    else:\n",
    "                        prediction = prediction     \n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    loss = criterion(prediction, masks)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                val_loss, val_num, val_iou, val_dice, val_f1 = 0, 0, 0, 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for imgs, masks in dataloader_val:\n",
    "                        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "                        prediction = model(imgs)\n",
    "\n",
    "                        if isinstance(prediction, dict):\n",
    "                            prediction = torch.Tensor(prediction['out'])\n",
    "                        elif isinstance(prediction, tuple):\n",
    "                            prediction = torch.Tensor(prediction[0])\n",
    "                        else:\n",
    "                            prediction = prediction    \n",
    "                \n",
    "                        loss = criterion(prediction, masks)\n",
    "                        val_loss += loss.item()\n",
    "                        val_num += len(imgs)\n",
    "                        val_iou += eval_segmentation(prediction, masks, metric='iou', batch_output=True).sum()\n",
    "                        val_dice += eval_segmentation(prediction, masks, metric='dice', batch_output=True).sum()\n",
    "                        val_f1 += eval_segmentation(prediction, masks, metric='f1', batch_output=True).sum()\n",
    "\n",
    "                # compute epoch-overall metric for val        \n",
    "                epoch_val_loss = val_loss/len(dataloader_val)\n",
    "                epoch_val_iou = (val_iou/val_num).item()\n",
    "                epoch_val_dice = (val_dice/val_num).item()\n",
    "                epoch_val_f1 = (val_f1/val_num).item()\n",
    "            \n",
    "                if epoch_val_dice >= state['best_val_dice']:\n",
    "                    state['best_val_dice'] = epoch_val_dice\n",
    "                    print(f'\\tSaving.. {epoch+1} of {epochs}, best_val_dice improved from {state['best_val_dice']:.4f} to {epoch_val_dice:.4f}')\n",
    "                    \n",
    "                                                   # /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder/{data_name}_{loss_name}.pth\n",
    "                    torch.save(model.state_dict(f\"/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/{data_name}_{loss_name}.pth\"))\n",
    "            \n",
    "                elif epoch - state['best_epoch'] > patience:\n",
    "                    print(f\"\\n\\tEarly stopping. Target criteria has not improved for {patience} epochs.\\n\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def we_want_to_do(data_names, model_name, loss_names, split_ratio=[0.6,0.2,0.2], base_random_seed=42, epochs=200, patience=50, BATCH_SIZE=8):\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    check_all_black_mask= []\n",
    "    \n",
    "    model = init_model(model_name)\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for data_name in data_names:\n",
    "        result[data_name] = {}\n",
    "        score_history = {}\n",
    "        loss_history = {}\n",
    "\n",
    "        if base_random_seed != None:\n",
    "            random_seed = base_random_seed\n",
    "            random.seed(random_seed)\n",
    "            np.random.seed(random_seed)\n",
    "            torch.manual_seed(random_seed)\n",
    "            torch.cuda.manual_seed(random_seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        _, _, test_images, _, _, test_labels = need_load_data(data_name, split_ratio=split_ratio, random_seed=random_seed)\n",
    "        custom_dataset_test = myDataSet(test_images, test_labels, transforms=test_transforms)\n",
    "        dataloader_test = torch.utils.data.DataLoader(custom_dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        \n",
    "        test_img_cnt = len(dataloader_test)\n",
    "\n",
    "        for loss_name in loss_names:\n",
    "                                            # /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder/{data_name}_{loss_name}.pth\", weights_only=True\n",
    "            print(f\"\\n /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/{data_name}_{loss_name}.pth 통과 전\\n\")\n",
    "            model.load_state_dict(torch.load(f\"/project/ahnailab/jsj0414/지역포함_0113/recorder_all_model/{data_name}_{loss_name}.pth\", weights_only=True))\n",
    "            print(f\"\\n/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/{data_name}_{loss_name}.pth 통과 \\n\")\n",
    "            model.to(DEVICE)\n",
    "\n",
    "            criterion = load_loss(loss_name)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 1e-8) \n",
    "        \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, (imgs, masks) in enumerate(dataloader_test):\n",
    "                    # 라벨이 없는 데이터가 존재. 이를 후보에서 제외.\n",
    "                    if masks.min() == masks.max():\n",
    "                        test_img_cnt -= 1\n",
    "                        with open(\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all_black_or_white_numb.txt\", \"a\") as f:\n",
    "                            f.write(f\"{data_name} : {i}\\n\")\n",
    "                        print(f\"\\n data : {data_name} img_numb : {i} \\n\")\n",
    "                        continue\n",
    "                    \n",
    "                    imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "                    prediction = model(imgs)\n",
    "        \n",
    "                    if isinstance(prediction, dict):\n",
    "                        prediction = torch.Tensor(prediction['out'])\n",
    "                    elif isinstance(prediction, tuple):\n",
    "                        prediction = torch.Tensor(prediction[0])\n",
    "                    else:\n",
    "                        prediction = prediction    \n",
    "                \n",
    "                    loss = criterion(prediction, masks)\n",
    "                    test_loss = loss.item()\n",
    "                    test_f1 = eval_segmentation(prediction, masks, metric='f1', batch_output=True).sum().item()\n",
    "        \n",
    "                    img_score = test_f1\n",
    "                    \n",
    "                    if not i in loss_history:\n",
    "                        loss_history[i] = [imgs.cpu(), masks.cpu()]\n",
    "                    loss_history[i].append((loss_name, prediction))\n",
    "                    # \n",
    "                    if not i in score_history:\n",
    "                        score_history[i] = 0\n",
    "                    score_history[i] += img_score\n",
    "\n",
    "                    del imgs, masks, prediction, test_f1, img_score\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "        # print(f'{data_name} : number of the test image is {test_img_cnt}')\n",
    "\n",
    "        score_lst = np.array(list(score_history.values()))\n",
    "\n",
    "        mid_res = {}\n",
    "        for _ in range(len(score_lst)):\n",
    "            pick_numb = np.argmax(score_lst)\n",
    "            \n",
    "            key_list = list(loss_history.keys())\n",
    "            img_numb = key_list[pick_numb]\n",
    "            \n",
    "            try:\n",
    "                mid_res[img_numb] = [loss_history[pick_numb]] \n",
    "                score_lst[pick_numb] = 0\n",
    "            \n",
    "            except KeyError:\n",
    "                with open(\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/missing_keys_VA.txt\", \"a\") as f:\n",
    "                    f.write(f\"{data_name} : {img_numb}\\n\")\n",
    "                    score_lst[pick_numb] = 0\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        result[data_name] = mid_res\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-idQzI2_6LM"
   },
   "source": [
    "# 데이터셋 클래스 생성\n",
    "> 해당 클래스는 이용하려는 이미지와 라벨의 모든 경로(/data/segmentation/...)의 리스트를 인자로 받는다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1663841626890,
     "user": {
      "displayName": "김동현",
      "userId": "12784596420296644443"
     },
     "user_tz": -540
    },
    "id": "QsLMJcor_6LL"
   },
   "outputs": [],
   "source": [
    "_size = 224, 224\n",
    "resize = transforms.Resize(_size, interpolation=0)\n",
    "\n",
    "# set your transforms \n",
    "train_transforms = transforms.Compose([\n",
    "                           transforms.Resize(_size, interpolation=0),\n",
    "                           transforms.RandomRotation(180),\n",
    "                           transforms.RandomHorizontalFlip(0.5),\n",
    "                           transforms.RandomCrop(_size, padding = 10), # needed after rotation (with original size)\n",
    "                       ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.Resize(_size, interpolation=0),\n",
    "                       ])\n",
    "\n",
    "# Save images to folder and create a custom dataloader that loads them from their path. More involved than method 1 but allows for greater flexibility\n",
    "# Requires 3 functions: __init__ to initialize the object, and __len__ and __get__item for pytorch purposes. More functions can be added as needed, but those 3 are necessary for it to function with pytorch\n",
    "class myDataSet(object):\n",
    "\n",
    "    def __init__(self, path_images, path_masks, transforms):\n",
    "        \"Initialization\"\n",
    "        self.all_path_images = sorted(path_images)\n",
    "        self.all_path_masks = sorted(path_masks)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Returns length of dataset\"\n",
    "        return len(self.all_path_images)  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Return next item of dataset\"\n",
    "        \n",
    "        if torch.is_tensor(index):        # 인덱스가 tensor 형태일 수 있으니 리스트 형태로 바꿔준다.\n",
    "            index = index.tolist()\n",
    "        \n",
    "        # Define path to current image and corresponding mask\n",
    "        path_img = self.all_path_images[index]\n",
    "        path_mask = self.all_path_masks[index]\n",
    "\n",
    "        # Load image and mask:\n",
    "        #     .jpeg has 3 channels, channels recorded last\n",
    "        #     .jpeg records values as intensities from 0 to 255\n",
    "        #     masks for some reason have values different to 0 or 255: 0, 1, 2, 3, 4, 5, 6, 7, 248, 249, 250, 251, 252, 253, 254, 255\n",
    "        img_bgr = cv2.imread(path_img) \n",
    "        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)  # cv2는 채널이 BGR로 저장된다 -> 출력할 때 RGB로 바꿔줘야함\n",
    "        img = img / 255  # 픽셀 값들을 0~1로 변환한다\n",
    "        \n",
    "        mask = cv2.imread(path_mask)[:, :, 0] / 255  # 마스크의 채널은 1개만 있으면 된다\n",
    "        mask = mask.round() # binarize to 0 or 1 (이진분류)\n",
    "        \n",
    "        # note, resizing happens inside transforms\n",
    "        \n",
    "        # convert to Tensors and fix the dimentions\n",
    "        img = torch.FloatTensor(np.transpose(img, [2, 0 ,1])) # Pytorch uses the channels in the first dimension\n",
    "        mask = torch.FloatTensor(mask).unsqueeze(0) # Adding channel dimension to label\n",
    "        \n",
    "        # apply transforms/augmentation to both image and mask together\n",
    "        sample = torch.cat((img, mask), 0) # insures that the same transform is applied\n",
    "        sample = self.transforms(sample)\n",
    "        img = sample[:img.shape[0], ...]\n",
    "        mask = sample[img.shape[0]:, ...]\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습과 이미지 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = pd.read_csv(\"/project/ahnailab/jsj0414/지역포함_0113/working_path/ISIC_path_change_ver_3.csv\")\n",
    "\n",
    "all_data = alls[\"type\"].unique().tolist() # \n",
    "\n",
    "all_models = 'FCBFormer'\n",
    "\n",
    "all_losses = [\"IoUWithContextualLoss\",\"DiceWithContextualLoss\",\"IoUFocalLoss\",\"DiceBCELoss\",\"IoULoss\",\"DiceLoss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 후 path 추가\n",
    "* 학습 후 주석처리하길 추천드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정 - LOSS등등\n",
    "df = pd.read_csv(\"/project/ahnailab/jsj0414/지역포함_0113/working_path/ISIC_path_change_ver_3.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "all_data = pd.read_csv(\"/project/ahnailab/jsj0414/지역포함_0113/all_result_analysis/all_data_0511.csv\")\n",
    "all_data = all_data[all_data[\"loss_name\"] != 'IoUWithTVLoss_tar']\n",
    "\n",
    "all_data = all_data[all_data[\"model_name\"].isna() != True].reset_index(drop = True)\n",
    "\n",
    "cols = all_losses\n",
    "\n",
    "data =df[\"type\"].unique().tolist()\n",
    "\n",
    "all_data = ['wound', 'CVC-ClinicDB', 'Kvasir-SEG', \n",
    "            'breast-cancer-benign', 'breast-cancer-malignant', 'ISIC']\n",
    "all_models = 'FCBFormer'\n",
    "loss = cols\n",
    "\n",
    "parameter = {\n",
    "    'data_names' : all_data, 'model_name' : all_models, 'loss_names' : loss,\n",
    "    'split_ratio' : [0.6, 0.2, 0.2], 'base_random_seed' : 42, 'epochs' : 200, 'patience' : 40, 'BATCH_SIZE' : 1\n",
    "    # ,'pass_num' : None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 현재 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameter = {\n",
    "#     'data_names' : ['ISIC',\n",
    "#  'breast-cancer-malignant',\n",
    "#  'breast-cancer-benign'], # begin부터 다시\n",
    "#     'model_name' : all_models,\n",
    "#     'loss_names' : [\n",
    "#                     \"IoUWithTVLoss\",\n",
    "#                      \"IoUWithTV_tar_Loss\",\n",
    "#                      \"IoUWithSSIMLoss\",\n",
    "#                       \"IoUWithEdgeLoss\",\n",
    "#                       \"IoUWithContextualLoss\",\n",
    "#                       \"DiceWithTVLoss_in\",\n",
    "#                       \"DiceWithTVLoss_tar\",\n",
    "#                      \"MultiScaleDiceLoss\",\n",
    "#                      \"DiceWithContextualLoss\",\n",
    "#                      \"DiceWithEdgeLoss\",\n",
    "#                      \"DiceWithSSIMLoss\",\n",
    "#                      \"FocalWithTVLoss_in\",\n",
    "#                       \"FocalWithTVLoss_tar\",\n",
    "#                      \"FocalWithGaussianLoss\",\n",
    "#                      \"FocalWithRegionWeighting\",\n",
    "#                      \"BCEWithTVLoss\",\n",
    "#                       \"BCEWithTVLoss_tar\",\n",
    "#                      \"BCEWithSSIMLoss\"\n",
    "#     ],\n",
    "#     'split_ratio' : [0.6, 0.2, 0.2], \n",
    "#     'base_random_seed' : 42,\n",
    "#     'epochs' : 200,\n",
    "#     'patience' : 40,\n",
    "#     'BATCH_SIZE' : 8\n",
    "# }\n",
    "\n",
    "# we_want_to_do(**parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **그리기 함수**\n",
    "- 예측 이미지에 시그모이드 적용\n",
    "- 시그모이드 적용에 이진 분류 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import resize as T_resize\n",
    "\n",
    "def draw(data_name, result, n_row, row_lim=(0, -1), col_lim=(0, -1), title=False, great_numb=0):\n",
    "    global axs\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    row_min, row_max = row_lim\n",
    "    col_min, col_max = col_lim\n",
    "# \n",
    "    datas = result[data_name][great_numb]\n",
    "    max_cols = axs.shape[1]\n",
    "    i = 0\n",
    "\n",
    "    for data in datas:\n",
    "        if isinstance(data, int):\n",
    "            continue\n",
    "        for d in data:\n",
    "            if i >= max_cols:\n",
    "                print(f\"Reached max cols ({max_cols}), stopping.\")\n",
    "                return\n",
    "\n",
    "            ax = axs[n_row, i]\n",
    "\n",
    "            # 0: 원본 이미지 (3채널)\n",
    "            if i == 0:\n",
    "                img = d.squeeze(0).permute(1,2,0)  # (H, W, 3)\n",
    "                img = T_resize(torch.tensor(img).permute(2, 0, 1), [224, 224])  # (3, H, W)\n",
    "                img = img.permute(1, 2, 0).numpy()  # 다시 (H, W, 3)로 복원\n",
    "                ax.imshow(img)\n",
    "\n",
    "            # 1: 마스크(1채널 → 3채널 보기용)\n",
    "            elif i == 1:\n",
    "                # 원본 1채널 마스크를 저장해둠 (H,W)\n",
    "                mask_tensor = d.squeeze(0)[0]      # (1,H,W) → (H,W)\n",
    "                \n",
    "                # resize로 224x224 맞추기 (crop 대신)\n",
    "                mask_tensor = T_resize(mask_tensor.unsqueeze(0), [224, 224]).squeeze(0)\n",
    "            \n",
    "                # 보기 편하게 3채널로 변환\n",
    "                mask_rgb = np.stack([mask_tensor.numpy()] * 3, axis=-1)\n",
    "                \n",
    "                ax.imshow(mask_rgb, cmap='gray')\n",
    "                ax.axis('off')\n",
    "                if title: ax.set_title(\"Mask\", fontsize=12)\n",
    "\n",
    "            # 2 이상: prediction → confusion map\n",
    "            else:\n",
    "                loss, pred = d\n",
    "                loss_name = loss.replace('Loss','')\n",
    "                \n",
    "                if \"Contextual\" in loss_name:\n",
    "                    loss_name = loss_name.split(\"With\")[0]+\"_Percepture\"\n",
    "                else:\n",
    "                    loss_name = loss_name\n",
    "                    \n",
    "                # 1) 분산 맵 & 이진화\n",
    "                var_map = pred\n",
    "                cut_line = 0.5\n",
    "                binary_pred = (var_map.squeeze() > cut_line).float()\n",
    "                # 동일하게 crop\n",
    "                binary_pred = T_resize(binary_pred.unsqueeze(0), [224, 224]).squeeze(0)\n",
    "\n",
    "                # 2) label binary\n",
    "                label_binary = mask_tensor.to(DEVICE) \n",
    "\n",
    "                # 3) confusion image 생성\n",
    "                h, w = binary_pred.shape\n",
    "                confusion = np.zeros((h, w, 3), dtype=float)\n",
    "                tp = ((binary_pred==1)&(label_binary==1)).cpu().numpy()\n",
    "                tn = ((binary_pred==0)&(label_binary==0)).cpu().numpy()\n",
    "                fp = ((binary_pred==1)&(label_binary==0)).cpu().numpy()\n",
    "                fn = ((binary_pred==0)&(label_binary==1)).cpu().numpy()\n",
    "                confusion[tp] = [1,1,1]  # white\n",
    "                confusion[fp] = [1,0,0]  # red\n",
    "                confusion[fn] = [0,0,1]  # blue\n",
    "                # tn는 검정(0,0,0)\n",
    "\n",
    "                ax.imshow(confusion)\n",
    "                ax.axis('off')\n",
    "                if title:\n",
    "                    ax.set_title(loss_name, fontsize=15)\n",
    "\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **결과**\n",
    "> 10/20/35 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_IoUWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_IoUWithContextualLoss.pth 통과 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " data : wound img_numb : 4 \n",
      "\n",
      "\n",
      " data : wound img_numb : 20 \n",
      "\n",
      "\n",
      " data : wound img_numb : 55 \n",
      "\n",
      "\n",
      " data : wound img_numb : 64 \n",
      "\n",
      "\n",
      " data : wound img_numb : 71 \n",
      "\n",
      "\n",
      " data : wound img_numb : 101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 119 \n",
      "\n",
      "\n",
      " data : wound img_numb : 122 \n",
      "\n",
      "\n",
      " data : wound img_numb : 133 \n",
      "\n",
      "\n",
      " data : wound img_numb : 155 \n",
      "\n",
      "\n",
      " data : wound img_numb : 196 \n",
      "\n",
      "\n",
      " data : wound img_numb : 199 \n",
      "\n",
      "\n",
      " data : wound img_numb : 211 \n",
      "\n",
      "\n",
      " data : wound img_numb : 218 \n",
      "\n",
      "\n",
      " data : wound img_numb : 235 \n",
      "\n",
      "\n",
      " data : wound img_numb : 254 \n",
      "\n",
      "\n",
      " data : wound img_numb : 261 \n",
      "\n",
      "\n",
      " data : wound img_numb : 305 \n",
      "\n",
      "\n",
      " data : wound img_numb : 306 \n",
      "\n",
      "\n",
      " data : wound img_numb : 326 \n",
      "\n",
      "\n",
      " data : wound img_numb : 328 \n",
      "\n",
      "\n",
      " data : wound img_numb : 345 \n",
      "\n",
      "\n",
      " data : wound img_numb : 350 \n",
      "\n",
      "\n",
      " data : wound img_numb : 355 \n",
      "\n",
      "\n",
      " data : wound img_numb : 368 \n",
      "\n",
      "\n",
      " data : wound img_numb : 399 \n",
      "\n",
      "\n",
      " data : wound img_numb : 415 \n",
      "\n",
      "\n",
      " data : wound img_numb : 451 \n",
      "\n",
      "\n",
      " data : wound img_numb : 466 \n",
      "\n",
      "\n",
      " data : wound img_numb : 492 \n",
      "\n",
      "\n",
      " data : wound img_numb : 495 \n",
      "\n",
      "\n",
      " data : wound img_numb : 522 \n",
      "\n",
      "\n",
      " data : wound img_numb : 536 \n",
      "\n",
      "\n",
      " data : wound img_numb : 588 \n",
      "\n",
      "\n",
      " data : wound img_numb : 592 \n",
      "\n",
      "\n",
      " data : wound img_numb : 626 \n",
      "\n",
      "\n",
      " data : wound img_numb : 630 \n",
      "\n",
      "\n",
      " data : wound img_numb : 706 \n",
      "\n",
      "\n",
      " data : wound img_numb : 733 \n",
      "\n",
      "\n",
      " data : wound img_numb : 735 \n",
      "\n",
      "\n",
      " data : wound img_numb : 754 \n",
      "\n",
      "\n",
      " data : wound img_numb : 760 \n",
      "\n",
      "\n",
      " data : wound img_numb : 763 \n",
      "\n",
      "\n",
      " data : wound img_numb : 769 \n",
      "\n",
      "\n",
      " data : wound img_numb : 771 \n",
      "\n",
      "\n",
      " data : wound img_numb : 779 \n",
      "\n",
      "\n",
      " data : wound img_numb : 780 \n",
      "\n",
      "\n",
      " data : wound img_numb : 795 \n",
      "\n",
      "\n",
      " data : wound img_numb : 811 \n",
      "\n",
      "\n",
      " data : wound img_numb : 823 \n",
      "\n",
      "\n",
      " data : wound img_numb : 825 \n",
      "\n",
      "\n",
      " data : wound img_numb : 830 \n",
      "\n",
      "\n",
      " data : wound img_numb : 854 \n",
      "\n",
      "\n",
      " data : wound img_numb : 878 \n",
      "\n",
      "\n",
      " data : wound img_numb : 883 \n",
      "\n",
      "\n",
      " data : wound img_numb : 896 \n",
      "\n",
      "\n",
      " data : wound img_numb : 906 \n",
      "\n",
      "\n",
      " data : wound img_numb : 921 \n",
      "\n",
      "\n",
      " data : wound img_numb : 925 \n",
      "\n",
      "\n",
      " data : wound img_numb : 939 \n",
      "\n",
      "\n",
      " data : wound img_numb : 980 \n",
      "\n",
      "\n",
      " data : wound img_numb : 984 \n",
      "\n",
      "\n",
      " data : wound img_numb : 985 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1073 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1085 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1099 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1102 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1107 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_DiceWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_DiceWithContextualLoss.pth 통과 \n",
      "\n",
      "\n",
      " data : wound img_numb : 4 \n",
      "\n",
      "\n",
      " data : wound img_numb : 20 \n",
      "\n",
      "\n",
      " data : wound img_numb : 55 \n",
      "\n",
      "\n",
      " data : wound img_numb : 64 \n",
      "\n",
      "\n",
      " data : wound img_numb : 71 \n",
      "\n",
      "\n",
      " data : wound img_numb : 101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 119 \n",
      "\n",
      "\n",
      " data : wound img_numb : 122 \n",
      "\n",
      "\n",
      " data : wound img_numb : 133 \n",
      "\n",
      "\n",
      " data : wound img_numb : 155 \n",
      "\n",
      "\n",
      " data : wound img_numb : 196 \n",
      "\n",
      "\n",
      " data : wound img_numb : 199 \n",
      "\n",
      "\n",
      " data : wound img_numb : 211 \n",
      "\n",
      "\n",
      " data : wound img_numb : 218 \n",
      "\n",
      "\n",
      " data : wound img_numb : 235 \n",
      "\n",
      "\n",
      " data : wound img_numb : 254 \n",
      "\n",
      "\n",
      " data : wound img_numb : 261 \n",
      "\n",
      "\n",
      " data : wound img_numb : 305 \n",
      "\n",
      "\n",
      " data : wound img_numb : 306 \n",
      "\n",
      "\n",
      " data : wound img_numb : 326 \n",
      "\n",
      "\n",
      " data : wound img_numb : 328 \n",
      "\n",
      "\n",
      " data : wound img_numb : 345 \n",
      "\n",
      "\n",
      " data : wound img_numb : 350 \n",
      "\n",
      "\n",
      " data : wound img_numb : 355 \n",
      "\n",
      "\n",
      " data : wound img_numb : 368 \n",
      "\n",
      "\n",
      " data : wound img_numb : 399 \n",
      "\n",
      "\n",
      " data : wound img_numb : 415 \n",
      "\n",
      "\n",
      " data : wound img_numb : 451 \n",
      "\n",
      "\n",
      " data : wound img_numb : 466 \n",
      "\n",
      "\n",
      " data : wound img_numb : 492 \n",
      "\n",
      "\n",
      " data : wound img_numb : 495 \n",
      "\n",
      "\n",
      " data : wound img_numb : 522 \n",
      "\n",
      "\n",
      " data : wound img_numb : 536 \n",
      "\n",
      "\n",
      " data : wound img_numb : 588 \n",
      "\n",
      "\n",
      " data : wound img_numb : 592 \n",
      "\n",
      "\n",
      " data : wound img_numb : 626 \n",
      "\n",
      "\n",
      " data : wound img_numb : 630 \n",
      "\n",
      "\n",
      " data : wound img_numb : 706 \n",
      "\n",
      "\n",
      " data : wound img_numb : 733 \n",
      "\n",
      "\n",
      " data : wound img_numb : 735 \n",
      "\n",
      "\n",
      " data : wound img_numb : 754 \n",
      "\n",
      "\n",
      " data : wound img_numb : 760 \n",
      "\n",
      "\n",
      " data : wound img_numb : 763 \n",
      "\n",
      "\n",
      " data : wound img_numb : 769 \n",
      "\n",
      "\n",
      " data : wound img_numb : 771 \n",
      "\n",
      "\n",
      " data : wound img_numb : 779 \n",
      "\n",
      "\n",
      " data : wound img_numb : 780 \n",
      "\n",
      "\n",
      " data : wound img_numb : 795 \n",
      "\n",
      "\n",
      " data : wound img_numb : 811 \n",
      "\n",
      "\n",
      " data : wound img_numb : 823 \n",
      "\n",
      "\n",
      " data : wound img_numb : 825 \n",
      "\n",
      "\n",
      " data : wound img_numb : 830 \n",
      "\n",
      "\n",
      " data : wound img_numb : 854 \n",
      "\n",
      "\n",
      " data : wound img_numb : 878 \n",
      "\n",
      "\n",
      " data : wound img_numb : 883 \n",
      "\n",
      "\n",
      " data : wound img_numb : 896 \n",
      "\n",
      "\n",
      " data : wound img_numb : 906 \n",
      "\n",
      "\n",
      " data : wound img_numb : 921 \n",
      "\n",
      "\n",
      " data : wound img_numb : 925 \n",
      "\n",
      "\n",
      " data : wound img_numb : 939 \n",
      "\n",
      "\n",
      " data : wound img_numb : 980 \n",
      "\n",
      "\n",
      " data : wound img_numb : 984 \n",
      "\n",
      "\n",
      " data : wound img_numb : 985 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1073 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1085 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1099 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1102 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1107 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_IoUFocalLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_IoUFocalLoss.pth 통과 \n",
      "\n",
      "\n",
      " data : wound img_numb : 4 \n",
      "\n",
      "\n",
      " data : wound img_numb : 20 \n",
      "\n",
      "\n",
      " data : wound img_numb : 55 \n",
      "\n",
      "\n",
      " data : wound img_numb : 64 \n",
      "\n",
      "\n",
      " data : wound img_numb : 71 \n",
      "\n",
      "\n",
      " data : wound img_numb : 101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 119 \n",
      "\n",
      "\n",
      " data : wound img_numb : 122 \n",
      "\n",
      "\n",
      " data : wound img_numb : 133 \n",
      "\n",
      "\n",
      " data : wound img_numb : 155 \n",
      "\n",
      "\n",
      " data : wound img_numb : 196 \n",
      "\n",
      "\n",
      " data : wound img_numb : 199 \n",
      "\n",
      "\n",
      " data : wound img_numb : 211 \n",
      "\n",
      "\n",
      " data : wound img_numb : 218 \n",
      "\n",
      "\n",
      " data : wound img_numb : 235 \n",
      "\n",
      "\n",
      " data : wound img_numb : 254 \n",
      "\n",
      "\n",
      " data : wound img_numb : 261 \n",
      "\n",
      "\n",
      " data : wound img_numb : 305 \n",
      "\n",
      "\n",
      " data : wound img_numb : 306 \n",
      "\n",
      "\n",
      " data : wound img_numb : 326 \n",
      "\n",
      "\n",
      " data : wound img_numb : 328 \n",
      "\n",
      "\n",
      " data : wound img_numb : 345 \n",
      "\n",
      "\n",
      " data : wound img_numb : 350 \n",
      "\n",
      "\n",
      " data : wound img_numb : 355 \n",
      "\n",
      "\n",
      " data : wound img_numb : 368 \n",
      "\n",
      "\n",
      " data : wound img_numb : 399 \n",
      "\n",
      "\n",
      " data : wound img_numb : 415 \n",
      "\n",
      "\n",
      " data : wound img_numb : 451 \n",
      "\n",
      "\n",
      " data : wound img_numb : 466 \n",
      "\n",
      "\n",
      " data : wound img_numb : 492 \n",
      "\n",
      "\n",
      " data : wound img_numb : 495 \n",
      "\n",
      "\n",
      " data : wound img_numb : 522 \n",
      "\n",
      "\n",
      " data : wound img_numb : 536 \n",
      "\n",
      "\n",
      " data : wound img_numb : 588 \n",
      "\n",
      "\n",
      " data : wound img_numb : 592 \n",
      "\n",
      "\n",
      " data : wound img_numb : 626 \n",
      "\n",
      "\n",
      " data : wound img_numb : 630 \n",
      "\n",
      "\n",
      " data : wound img_numb : 706 \n",
      "\n",
      "\n",
      " data : wound img_numb : 733 \n",
      "\n",
      "\n",
      " data : wound img_numb : 735 \n",
      "\n",
      "\n",
      " data : wound img_numb : 754 \n",
      "\n",
      "\n",
      " data : wound img_numb : 760 \n",
      "\n",
      "\n",
      " data : wound img_numb : 763 \n",
      "\n",
      "\n",
      " data : wound img_numb : 769 \n",
      "\n",
      "\n",
      " data : wound img_numb : 771 \n",
      "\n",
      "\n",
      " data : wound img_numb : 779 \n",
      "\n",
      "\n",
      " data : wound img_numb : 780 \n",
      "\n",
      "\n",
      " data : wound img_numb : 795 \n",
      "\n",
      "\n",
      " data : wound img_numb : 811 \n",
      "\n",
      "\n",
      " data : wound img_numb : 823 \n",
      "\n",
      "\n",
      " data : wound img_numb : 825 \n",
      "\n",
      "\n",
      " data : wound img_numb : 830 \n",
      "\n",
      "\n",
      " data : wound img_numb : 854 \n",
      "\n",
      "\n",
      " data : wound img_numb : 878 \n",
      "\n",
      "\n",
      " data : wound img_numb : 883 \n",
      "\n",
      "\n",
      " data : wound img_numb : 896 \n",
      "\n",
      "\n",
      " data : wound img_numb : 906 \n",
      "\n",
      "\n",
      " data : wound img_numb : 921 \n",
      "\n",
      "\n",
      " data : wound img_numb : 925 \n",
      "\n",
      "\n",
      " data : wound img_numb : 939 \n",
      "\n",
      "\n",
      " data : wound img_numb : 980 \n",
      "\n",
      "\n",
      " data : wound img_numb : 984 \n",
      "\n",
      "\n",
      " data : wound img_numb : 985 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1073 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1085 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1099 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1102 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1107 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_DiceBCELoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_DiceBCELoss.pth 통과 \n",
      "\n",
      "\n",
      " data : wound img_numb : 4 \n",
      "\n",
      "\n",
      " data : wound img_numb : 20 \n",
      "\n",
      "\n",
      " data : wound img_numb : 55 \n",
      "\n",
      "\n",
      " data : wound img_numb : 64 \n",
      "\n",
      "\n",
      " data : wound img_numb : 71 \n",
      "\n",
      "\n",
      " data : wound img_numb : 101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 119 \n",
      "\n",
      "\n",
      " data : wound img_numb : 122 \n",
      "\n",
      "\n",
      " data : wound img_numb : 133 \n",
      "\n",
      "\n",
      " data : wound img_numb : 155 \n",
      "\n",
      "\n",
      " data : wound img_numb : 196 \n",
      "\n",
      "\n",
      " data : wound img_numb : 199 \n",
      "\n",
      "\n",
      " data : wound img_numb : 211 \n",
      "\n",
      "\n",
      " data : wound img_numb : 218 \n",
      "\n",
      "\n",
      " data : wound img_numb : 235 \n",
      "\n",
      "\n",
      " data : wound img_numb : 254 \n",
      "\n",
      "\n",
      " data : wound img_numb : 261 \n",
      "\n",
      "\n",
      " data : wound img_numb : 305 \n",
      "\n",
      "\n",
      " data : wound img_numb : 306 \n",
      "\n",
      "\n",
      " data : wound img_numb : 326 \n",
      "\n",
      "\n",
      " data : wound img_numb : 328 \n",
      "\n",
      "\n",
      " data : wound img_numb : 345 \n",
      "\n",
      "\n",
      " data : wound img_numb : 350 \n",
      "\n",
      "\n",
      " data : wound img_numb : 355 \n",
      "\n",
      "\n",
      " data : wound img_numb : 368 \n",
      "\n",
      "\n",
      " data : wound img_numb : 399 \n",
      "\n",
      "\n",
      " data : wound img_numb : 415 \n",
      "\n",
      "\n",
      " data : wound img_numb : 451 \n",
      "\n",
      "\n",
      " data : wound img_numb : 466 \n",
      "\n",
      "\n",
      " data : wound img_numb : 492 \n",
      "\n",
      "\n",
      " data : wound img_numb : 495 \n",
      "\n",
      "\n",
      " data : wound img_numb : 522 \n",
      "\n",
      "\n",
      " data : wound img_numb : 536 \n",
      "\n",
      "\n",
      " data : wound img_numb : 588 \n",
      "\n",
      "\n",
      " data : wound img_numb : 592 \n",
      "\n",
      "\n",
      " data : wound img_numb : 626 \n",
      "\n",
      "\n",
      " data : wound img_numb : 630 \n",
      "\n",
      "\n",
      " data : wound img_numb : 706 \n",
      "\n",
      "\n",
      " data : wound img_numb : 733 \n",
      "\n",
      "\n",
      " data : wound img_numb : 735 \n",
      "\n",
      "\n",
      " data : wound img_numb : 754 \n",
      "\n",
      "\n",
      " data : wound img_numb : 760 \n",
      "\n",
      "\n",
      " data : wound img_numb : 763 \n",
      "\n",
      "\n",
      " data : wound img_numb : 769 \n",
      "\n",
      "\n",
      " data : wound img_numb : 771 \n",
      "\n",
      "\n",
      " data : wound img_numb : 779 \n",
      "\n",
      "\n",
      " data : wound img_numb : 780 \n",
      "\n",
      "\n",
      " data : wound img_numb : 795 \n",
      "\n",
      "\n",
      " data : wound img_numb : 811 \n",
      "\n",
      "\n",
      " data : wound img_numb : 823 \n",
      "\n",
      "\n",
      " data : wound img_numb : 825 \n",
      "\n",
      "\n",
      " data : wound img_numb : 830 \n",
      "\n",
      "\n",
      " data : wound img_numb : 854 \n",
      "\n",
      "\n",
      " data : wound img_numb : 878 \n",
      "\n",
      "\n",
      " data : wound img_numb : 883 \n",
      "\n",
      "\n",
      " data : wound img_numb : 896 \n",
      "\n",
      "\n",
      " data : wound img_numb : 906 \n",
      "\n",
      "\n",
      " data : wound img_numb : 921 \n",
      "\n",
      "\n",
      " data : wound img_numb : 925 \n",
      "\n",
      "\n",
      " data : wound img_numb : 939 \n",
      "\n",
      "\n",
      " data : wound img_numb : 980 \n",
      "\n",
      "\n",
      " data : wound img_numb : 984 \n",
      "\n",
      "\n",
      " data : wound img_numb : 985 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1073 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1085 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1099 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1102 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1107 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_IoULoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_IoULoss.pth 통과 \n",
      "\n",
      "\n",
      " data : wound img_numb : 4 \n",
      "\n",
      "\n",
      " data : wound img_numb : 20 \n",
      "\n",
      "\n",
      " data : wound img_numb : 55 \n",
      "\n",
      "\n",
      " data : wound img_numb : 64 \n",
      "\n",
      "\n",
      " data : wound img_numb : 71 \n",
      "\n",
      "\n",
      " data : wound img_numb : 101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 119 \n",
      "\n",
      "\n",
      " data : wound img_numb : 122 \n",
      "\n",
      "\n",
      " data : wound img_numb : 133 \n",
      "\n",
      "\n",
      " data : wound img_numb : 155 \n",
      "\n",
      "\n",
      " data : wound img_numb : 196 \n",
      "\n",
      "\n",
      " data : wound img_numb : 199 \n",
      "\n",
      "\n",
      " data : wound img_numb : 211 \n",
      "\n",
      "\n",
      " data : wound img_numb : 218 \n",
      "\n",
      "\n",
      " data : wound img_numb : 235 \n",
      "\n",
      "\n",
      " data : wound img_numb : 254 \n",
      "\n",
      "\n",
      " data : wound img_numb : 261 \n",
      "\n",
      "\n",
      " data : wound img_numb : 305 \n",
      "\n",
      "\n",
      " data : wound img_numb : 306 \n",
      "\n",
      "\n",
      " data : wound img_numb : 326 \n",
      "\n",
      "\n",
      " data : wound img_numb : 328 \n",
      "\n",
      "\n",
      " data : wound img_numb : 345 \n",
      "\n",
      "\n",
      " data : wound img_numb : 350 \n",
      "\n",
      "\n",
      " data : wound img_numb : 355 \n",
      "\n",
      "\n",
      " data : wound img_numb : 368 \n",
      "\n",
      "\n",
      " data : wound img_numb : 399 \n",
      "\n",
      "\n",
      " data : wound img_numb : 415 \n",
      "\n",
      "\n",
      " data : wound img_numb : 451 \n",
      "\n",
      "\n",
      " data : wound img_numb : 466 \n",
      "\n",
      "\n",
      " data : wound img_numb : 492 \n",
      "\n",
      "\n",
      " data : wound img_numb : 495 \n",
      "\n",
      "\n",
      " data : wound img_numb : 522 \n",
      "\n",
      "\n",
      " data : wound img_numb : 536 \n",
      "\n",
      "\n",
      " data : wound img_numb : 588 \n",
      "\n",
      "\n",
      " data : wound img_numb : 592 \n",
      "\n",
      "\n",
      " data : wound img_numb : 626 \n",
      "\n",
      "\n",
      " data : wound img_numb : 630 \n",
      "\n",
      "\n",
      " data : wound img_numb : 706 \n",
      "\n",
      "\n",
      " data : wound img_numb : 733 \n",
      "\n",
      "\n",
      " data : wound img_numb : 735 \n",
      "\n",
      "\n",
      " data : wound img_numb : 754 \n",
      "\n",
      "\n",
      " data : wound img_numb : 760 \n",
      "\n",
      "\n",
      " data : wound img_numb : 763 \n",
      "\n",
      "\n",
      " data : wound img_numb : 769 \n",
      "\n",
      "\n",
      " data : wound img_numb : 771 \n",
      "\n",
      "\n",
      " data : wound img_numb : 779 \n",
      "\n",
      "\n",
      " data : wound img_numb : 780 \n",
      "\n",
      "\n",
      " data : wound img_numb : 795 \n",
      "\n",
      "\n",
      " data : wound img_numb : 811 \n",
      "\n",
      "\n",
      " data : wound img_numb : 823 \n",
      "\n",
      "\n",
      " data : wound img_numb : 825 \n",
      "\n",
      "\n",
      " data : wound img_numb : 830 \n",
      "\n",
      "\n",
      " data : wound img_numb : 854 \n",
      "\n",
      "\n",
      " data : wound img_numb : 878 \n",
      "\n",
      "\n",
      " data : wound img_numb : 883 \n",
      "\n",
      "\n",
      " data : wound img_numb : 896 \n",
      "\n",
      "\n",
      " data : wound img_numb : 906 \n",
      "\n",
      "\n",
      " data : wound img_numb : 921 \n",
      "\n",
      "\n",
      " data : wound img_numb : 925 \n",
      "\n",
      "\n",
      " data : wound img_numb : 939 \n",
      "\n",
      "\n",
      " data : wound img_numb : 980 \n",
      "\n",
      "\n",
      " data : wound img_numb : 984 \n",
      "\n",
      "\n",
      " data : wound img_numb : 985 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1073 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1085 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1099 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1102 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1107 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_DiceLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/wound_DiceLoss.pth 통과 \n",
      "\n",
      "\n",
      " data : wound img_numb : 4 \n",
      "\n",
      "\n",
      " data : wound img_numb : 20 \n",
      "\n",
      "\n",
      " data : wound img_numb : 55 \n",
      "\n",
      "\n",
      " data : wound img_numb : 64 \n",
      "\n",
      "\n",
      " data : wound img_numb : 71 \n",
      "\n",
      "\n",
      " data : wound img_numb : 101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 119 \n",
      "\n",
      "\n",
      " data : wound img_numb : 122 \n",
      "\n",
      "\n",
      " data : wound img_numb : 133 \n",
      "\n",
      "\n",
      " data : wound img_numb : 155 \n",
      "\n",
      "\n",
      " data : wound img_numb : 196 \n",
      "\n",
      "\n",
      " data : wound img_numb : 199 \n",
      "\n",
      "\n",
      " data : wound img_numb : 211 \n",
      "\n",
      "\n",
      " data : wound img_numb : 218 \n",
      "\n",
      "\n",
      " data : wound img_numb : 235 \n",
      "\n",
      "\n",
      " data : wound img_numb : 254 \n",
      "\n",
      "\n",
      " data : wound img_numb : 261 \n",
      "\n",
      "\n",
      " data : wound img_numb : 305 \n",
      "\n",
      "\n",
      " data : wound img_numb : 306 \n",
      "\n",
      "\n",
      " data : wound img_numb : 326 \n",
      "\n",
      "\n",
      " data : wound img_numb : 328 \n",
      "\n",
      "\n",
      " data : wound img_numb : 345 \n",
      "\n",
      "\n",
      " data : wound img_numb : 350 \n",
      "\n",
      "\n",
      " data : wound img_numb : 355 \n",
      "\n",
      "\n",
      " data : wound img_numb : 368 \n",
      "\n",
      "\n",
      " data : wound img_numb : 399 \n",
      "\n",
      "\n",
      " data : wound img_numb : 415 \n",
      "\n",
      "\n",
      " data : wound img_numb : 451 \n",
      "\n",
      "\n",
      " data : wound img_numb : 466 \n",
      "\n",
      "\n",
      " data : wound img_numb : 492 \n",
      "\n",
      "\n",
      " data : wound img_numb : 495 \n",
      "\n",
      "\n",
      " data : wound img_numb : 522 \n",
      "\n",
      "\n",
      " data : wound img_numb : 536 \n",
      "\n",
      "\n",
      " data : wound img_numb : 588 \n",
      "\n",
      "\n",
      " data : wound img_numb : 592 \n",
      "\n",
      "\n",
      " data : wound img_numb : 626 \n",
      "\n",
      "\n",
      " data : wound img_numb : 630 \n",
      "\n",
      "\n",
      " data : wound img_numb : 706 \n",
      "\n",
      "\n",
      " data : wound img_numb : 733 \n",
      "\n",
      "\n",
      " data : wound img_numb : 735 \n",
      "\n",
      "\n",
      " data : wound img_numb : 754 \n",
      "\n",
      "\n",
      " data : wound img_numb : 760 \n",
      "\n",
      "\n",
      " data : wound img_numb : 763 \n",
      "\n",
      "\n",
      " data : wound img_numb : 769 \n",
      "\n",
      "\n",
      " data : wound img_numb : 771 \n",
      "\n",
      "\n",
      " data : wound img_numb : 779 \n",
      "\n",
      "\n",
      " data : wound img_numb : 780 \n",
      "\n",
      "\n",
      " data : wound img_numb : 795 \n",
      "\n",
      "\n",
      " data : wound img_numb : 811 \n",
      "\n",
      "\n",
      " data : wound img_numb : 823 \n",
      "\n",
      "\n",
      " data : wound img_numb : 825 \n",
      "\n",
      "\n",
      " data : wound img_numb : 830 \n",
      "\n",
      "\n",
      " data : wound img_numb : 854 \n",
      "\n",
      "\n",
      " data : wound img_numb : 878 \n",
      "\n",
      "\n",
      " data : wound img_numb : 883 \n",
      "\n",
      "\n",
      " data : wound img_numb : 896 \n",
      "\n",
      "\n",
      " data : wound img_numb : 906 \n",
      "\n",
      "\n",
      " data : wound img_numb : 921 \n",
      "\n",
      "\n",
      " data : wound img_numb : 925 \n",
      "\n",
      "\n",
      " data : wound img_numb : 939 \n",
      "\n",
      "\n",
      " data : wound img_numb : 980 \n",
      "\n",
      "\n",
      " data : wound img_numb : 984 \n",
      "\n",
      "\n",
      " data : wound img_numb : 985 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1073 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1085 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1099 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1101 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1102 \n",
      "\n",
      "\n",
      " data : wound img_numb : 1107 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_IoUWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_IoUWithContextualLoss.pth 통과 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_DiceWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_DiceWithContextualLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_IoUFocalLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_IoUFocalLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_DiceBCELoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_DiceBCELoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_IoULoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_IoULoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_DiceLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/CVC-ClinicDB_DiceLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_IoUWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_IoUWithContextualLoss.pth 통과 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_DiceWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_DiceWithContextualLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_IoUFocalLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_IoUFocalLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_DiceBCELoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_DiceBCELoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_IoULoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_IoULoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_DiceLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/Kvasir-SEG_DiceLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_IoUWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_IoUWithContextualLoss.pth 통과 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_DiceWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_DiceWithContextualLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_IoUFocalLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_IoUFocalLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_DiceBCELoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_DiceBCELoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_IoULoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_IoULoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_DiceLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-benign_DiceLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_IoUWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_IoUWithContextualLoss.pth 통과 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_DiceWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_DiceWithContextualLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_IoUFocalLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_IoUFocalLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_DiceBCELoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_DiceBCELoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_IoULoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_IoULoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_DiceLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/breast-cancer-malignant_DiceLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_IoUWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_IoUWithContextualLoss.pth 통과 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jsj0414/.conda/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_DiceWithContextualLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_DiceWithContextualLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_IoUFocalLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_IoUFocalLoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_DiceBCELoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_DiceBCELoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_IoULoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_IoULoss.pth 통과 \n",
      "\n",
      "\n",
      " /project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_DiceLoss.pth 통과 전\n",
      "\n",
      "\n",
      "/project/ahnailab/jsj0414/지역포함_0113/working_path/model_recorder_all/ISIC_DiceLoss.pth 통과 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "need_parameter = {\n",
    "    'data_names' : all_data, 'model_name' : all_models, 'loss_names' : all_losses,\n",
    "    'split_ratio' : [1.0], 'base_random_seed' : 42, 'epochs' : 200, 'patience' : 40, 'BATCH_SIZE' : 1\n",
    "    # ,'pass_num' : None\n",
    "}\n",
    "\n",
    "need_result = we_want_to_do(**need_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장된 결과 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 파일로 저장\n",
    "with open('/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all_result_f1_real_last.pkl', 'wb') as f:\n",
    "    pickle.dump(need_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all_result_f1_real_last.pkl', 'rb') as f:\n",
    "    loaded_dict_real_last = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이전 결과들(뒷 결과들과 상관 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 파일로 저장\n",
    "# with open('/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all_result_f1_3.pkl', 'wb') as f:\n",
    "#     pickle.dump(need_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all_result_f1_3.pkl', 'rb') as f:\n",
    "#     loaded_dict_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all_result_f1_sido_2.pkl', 'rb') as f:\n",
    "#     loaded_dict_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (완료)wound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_keys = list(loaded_dict_real_last[\"wound\"].keys())\n",
    "key_all_len = len(all_keys)\n",
    "for i in range(key_all_len):\n",
    "    key = all_keys[i]\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 5), squeeze=False)\n",
    "\n",
    "    great_num=key\n",
    "    \n",
    "    _ = plt.suptitle(f'the pass_num is {i}', fontsize=30,ㅇy=0.9)\n",
    "    draw('wound', loaded_dict_real_last, n_row=0, row_lim=(25,105), col_lim=(0,80), title=True, great_numb=great_num)  # great_numb=이미지 키번호 넣어야한다.\n",
    "    \n",
    "    # ====== 간격 조절 ======\n",
    "    plt.tight_layout(h_pad=0.1, w_pad=0.5)  # h_pad, w_pad로 간격 조절\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9)  # 제목이랑 subplot 사이 여백 조정\n",
    "    plt.savefig(f\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all/wound/F1_{i}_img_pred.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVC-ClinicDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_keys = list(loaded_dict_real_last[\"CVC-ClinicDB\"].keys())\n",
    "key_all_len = len(all_keys)\n",
    "for i in range(key_all_len):\n",
    "    key = all_keys[i]\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 5), squeeze=False)\n",
    "\n",
    "    great_num=key\n",
    "    \n",
    "    _ = plt.suptitle(f'the pass_num is {i}', fontsize=30, y=0.9)\n",
    "    draw('CVC-ClinicDB', loaded_dict_real_last, n_row=0, row_lim=(25,105), col_lim=(0,80), title=True, great_numb=great_num)  \n",
    "    \n",
    "    # ====== 간격 조절 ======\n",
    "    plt.tight_layout(h_pad=0.1, w_pad=0.5)  # h_pad, w_pad로 간격 조절\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9)  # 제목이랑 subplot 사이 여백 조정\n",
    "    plt.savefig(f\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all/CVC/F1_{i}_img_pred.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kvasir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(loaded_dict_real_last[\"Kvasir-SEG\"].keys())\n",
    "key_all_len = len(all_keys)\n",
    "for i in range(key_all_len):\n",
    "    key = all_keys[i]\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 5), squeeze=False)\n",
    "\n",
    "    great_num=key\n",
    "    \n",
    "    _ = plt.suptitle(f'the pass_num is {i}', fontsize=30, y=0.9)\n",
    "    draw('Kvasir-SEG', loaded_dict_real_last, n_row=0, row_lim=(25,105), col_lim=(0,80), title=True, great_numb=great_num) # great_numb=이미지 키번호 넣어야한다.\n",
    "    \n",
    "    # ====== 간격 조절 ======\n",
    "    plt.tight_layout(h_pad=0.1, w_pad=0.5)  # h_pad, w_pad로 간격 조절\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9)  # 제목이랑 subplot 사이 여백 조정\n",
    "    plt.savefig(f\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all/Kvasir/F1_{i}_img_pred.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## breast-cancer-benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_keys = list(loaded_dict_real_last[\"breast-cancer-benign\"].keys())\n",
    "key_all_len = len(all_keys)\n",
    "for i in range(key_all_len):\n",
    "    key = all_keys[i]\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 5), squeeze=False)\n",
    "\n",
    "    great_num=key\n",
    "    \n",
    "    _ = plt.suptitle(f'the pass_num is {i}', fontsize=30, y=0.9)\n",
    "    draw('breast-cancer-benign', loaded_dict_real_last, n_row=0, row_lim=(25,105), col_lim=(0,80), title=True, great_numb=great_num)  # great_numb=이미지 키번호 넣어야한다.\n",
    "    \n",
    "    # ====== 간격 조절 ======\n",
    "    plt.tight_layout(h_pad=0.1, w_pad=0.5)  # h_pad, w_pad로 간격 조절\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9)  # 제목이랑 subplot 사이 여백 조정\n",
    "    plt.savefig(f\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all/begin/F1_{i}_img_pred.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## breast-cancer-malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(loaded_dict_real_last[\"breast-cancer-malignant\"].keys())\n",
    "key_all_len = len(all_keys)\n",
    "for i in range(key_all_len):\n",
    "    key = all_keys[i]\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 5), squeeze=False)\n",
    "\n",
    "    great_num=key\n",
    "    \n",
    "    _ = plt.suptitle(f'the pass_num is {i}', fontsize=30, y=0.9)\n",
    "    draw('breast-cancer-malignant', loaded_dict_real_last, n_row=0, row_lim=(25,105), col_lim=(0,80), title=True, great_numb=great_num)  # great_numb=이미지 키번호 넣어야한다.\n",
    "    \n",
    "    # ====== 간격 조절 ======\n",
    "    plt.tight_layout(h_pad=0.1, w_pad=0.5)  # h_pad, w_pad로 간격 조절\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9)  # 제목이랑 subplot 사이 여백 조정\n",
    "    plt.savefig(f\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all/malignant/F1_{i}_img_pred.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_keys = list(loaded_dict_real_last[\"ISIC\"].keys())\n",
    "key_all_len = len(all_keys)\n",
    "for i in range(key_all_len):\n",
    "    key = all_keys[i]\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 5), squeeze=False)\n",
    "\n",
    "    great_num=key\n",
    "    \n",
    "    _ = plt.suptitle(f'the pass_num is {i}', fontsize=30, y=0.9)\n",
    "    draw('ISIC', loaded_dict_real_last, n_row=0, row_lim=(25,105), col_lim=(0,80), title=True, great_numb=great_num)  \n",
    "    \n",
    "    # ====== 간격 조절 ======\n",
    "    plt.tight_layout(h_pad=0.1, w_pad=0.5)  # h_pad, w_pad로 간격 조절\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9)  # 제목이랑 subplot 사이 여백 조정\n",
    "    plt.savefig(f\"/project/ahnailab/jsj0414/지역포함_0113/VAR_img/2차시도/all/ISIC/F1_{i}_img_pred.png\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
